{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming Data to Unlock Its Latent Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "path = 'data'\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_data(url, name, path='data'):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "    response = requests.get(url)\n",
    "    with open(os.path.join(path, name), 'wb') as f:\n",
    "        f.write(response.content)\n",
    "        \n",
    "    z = zipfile.ZipFile(os.path.join(path, 'vehicles.zip'))\n",
    "    z.extractall(path)\n",
    "\n",
    "VEHICLES = 'http://www.fueleconomy.gov/feg/epadata/vehicles.csv.zip'\n",
    "\n",
    "download_data(VEHICLES, 'vehicles.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vehicles = pd.read_csv(os.path.join(path, 'vehicles.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and Reorganize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "select_columns = ['make', 'model', 'year', 'displ', 'cylinders', 'trany', 'drive', 'VClass','fuelType',  \n",
    "                 'barrels08', 'city08', 'highway08', 'comb08', 'co2TailpipeGpm', 'fuelCost08']\n",
    "\n",
    "vehicles = vehicles[select_columns][vehicles.year <= 2016].drop_duplicates().dropna()\n",
    "vehicles = vehicles.sort_values(['make', 'model', 'year'])\n",
    "vehicles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Category Aggregations\n",
    "\n",
    "Hint: Look for object fields that have many categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unique_col_values(df):\n",
    "    for column in df:\n",
    "        print(str(df[column].name) + \" | \" + str(len(df[column].unique())) + \" | \" + str(df[column].dtype))\n",
    "\n",
    "unique_col_values(vehicles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create new trantype field that specifies whether the vehicle is Automatic or Manual. \n",
    "vehicles.loc[vehicles.trany.str[0] == 'A', 'trantype'] = 'Automatic'\n",
    "vehicles.loc[vehicles.trany.str[0] == 'M', 'trantype'] = 'Manual'\n",
    "\n",
    "#Create new model_type field that parses the model type from the model field. \n",
    "vehicles['model_type'] = vehicles.make + \" \" + vehicles.model.str.split().str.get(0)\n",
    "\n",
    "#Create new category field that rolls up VClass into more general categories. \n",
    "small = ['Compact Cars','Subcompact Cars','Two Seaters','Minicompact Cars']\n",
    "midsize = ['Midsize Cars']\n",
    "large = ['Large Cars']\n",
    "\n",
    "vehicles.loc[vehicles.VClass.isin(small), 'category'] = 'Small Cars'\n",
    "vehicles.loc[vehicles.VClass.isin(midsize), 'category'] = 'Midsize Cars'\n",
    "vehicles.loc[vehicles.VClass.isin(large), 'category'] = 'Large Cars'\n",
    "vehicles.loc[vehicles.VClass.str.contains('Station'), 'category'] = 'Station Wagons'\n",
    "vehicles.loc[vehicles.VClass.str.contains('Pickup'), 'category'] = 'Pickup Trucks'\n",
    "vehicles.loc[vehicles.VClass.str.contains('Special Purpose'), 'category'] = 'Special Purpose'\n",
    "vehicles.loc[vehicles.VClass.str.contains('Sport Utility'), 'category'] = 'Sport Utility'\n",
    "vehicles.loc[(vehicles.VClass.str.contains('van')) | (vehicles.VClass.str.contains('van')),\n",
    "               'category'] = 'Vans & Minivans'\n",
    "\n",
    "#Create new fuel_category field that rolls up fuelType into more general categories. \n",
    "vehicles['fuel_category'] = ''\n",
    "gas = ['Regular', 'Premium', 'Midgrade']\n",
    "vehicles.loc[vehicles.fuelType.isin(gas), 'fuel_category'] = 'Gasoline'\n",
    "vehicles.loc[vehicles.fuelType == 'Diesel', 'fuel_category'] = 'Diesel'\n",
    "vehicles.loc[vehicles.fuel_category == '', 'fuel_category'] = 'Alternative/Hybrid'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Categorical Fields from Continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "engine_categories = ['Very Small Engine', 'Small Engine','Moderate Engine', \n",
    "                     'Large Engine', 'Very Large Engine']\n",
    "vehicles['engine_size'] = pd.qcut(vehicles.displ, 5, engine_categories)\n",
    "\n",
    "efficiency_categories = ['Very Low Efficiency', 'Low Efficiency', 'Moderate Efficiency',\n",
    "                        'High Efficiency', 'Very High Efficiency']\n",
    "vehicles['fuel_efficiency'] = pd.qcut(vehicles.comb08, 5, efficiency_categories)\n",
    "\n",
    "emmission_categories = ['Very Low Emmissions', 'Low Emmissions', 'Moderate Emmissions',\n",
    "                       'High Emmissions', 'Very High Emmissions']\n",
    "vehicles['emmission'] = pd.qcut(vehicles.co2TailpipeGpm, 5, emmission_categories)\n",
    "\n",
    "fuelcost_categories = ['Very Low Fuel Cost', 'Low Fuel Cost', 'Moderate Fuel Cost',\n",
    "                      'High Fuel Cost', 'Very High Fuel Cost']\n",
    "vehicles['fuel_cost'] = pd.qcut(vehicles.fuelCost08, 5, fuelcost_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster to Create Additional Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vehicles_numeric = vehicles._get_numeric_data()\n",
    "del vehicles_numeric['year']\n",
    "\n",
    "vehicles_numeric_norm = vehicles_numeric.apply(lambda x: (x / x.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "model = KMeans(n_clusters=8)\n",
    "clusters = model.fit_predict(vehicles_numeric_norm)\n",
    "vehicles_numeric_norm['cluster'] = clusters\n",
    "\n",
    "cluster_means = vehicles_numeric_norm.groupby(['cluster'], as_index=False).mean()\n",
    "cluster_columns = ['displ','cylinders','barrels08','city08','highway08','comb08','co2TailpipeGpm','fuelCost08']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "sns.heatmap(cluster_means[cluster_columns], annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=4)\n",
    "clusters = model.fit_predict(vehicles_numeric_norm)\n",
    "vehicles_numeric_norm['cluster'] = clusters\n",
    "\n",
    "cluster_means = vehicles_numeric_norm.groupby(['cluster'], as_index=False).mean()\n",
    "cluster_columns = ['displ','cylinders','barrels08','city08','highway08','comb08','co2TailpipeGpm','fuelCost08']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "sns.heatmap(cluster_means[cluster_columns], annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vehicles['cluster'] = clusters\n",
    "vehicles['cluster'][vehicles['cluster']==0] = 'Small Very Efficient'\n",
    "vehicles['cluster'][vehicles['cluster']==1] = 'Large Inefficient'\n",
    "vehicles['cluster'][vehicles['cluster']==2] = 'Midsized Balanced'\n",
    "vehicles['cluster'][vehicles['cluster']==3] = 'Small Moderately Efficient'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate and Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_barchart(df, year, group_field, length, width):\n",
    "    df_year = df[df.year == year]\n",
    "    grouped = df_year.groupby(group_field, as_index=False).size().reset_index()\n",
    "    grouped = grouped.rename(columns={0: 'count'}).sort_values('count', ascending=False)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(width,length))\n",
    "    sns.barplot(x=\"count\", y=group_field, data=grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_barchart(vehicles, 2016, 'category', 6,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_barchart(vehicles, 1985, 'category', 6,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_barchart(vehicles, 2016, 'engine_size', 6, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_barchart(vehicles, 2016, 'fuel_efficiency', 6, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_barchart(vehicles, 2016, 'cluster', 6,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_barchart(vehicles, 2016, 'make', 12, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Details with Pivoting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pivot_heatmap(df, year, rows, columns, values, width, length):\n",
    "    df_year = df[df.year == year]\n",
    "    df_pivot = df_year.pivot_table(values=values, index=rows, columns=columns, \n",
    "                                   aggfunc=np.size).dropna(axis=0, how='all')\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(width,length))\n",
    "    sns.heatmap(df_pivot, annot=True, fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pivot_heatmap(vehicles, 2016, 'fuel_efficiency','engine_size','comb08',15, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pivot_heatmap(vehicles, 1985, 'fuel_efficiency','engine_size','comb08',15, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pivot_heatmap(vehicles, 2016, 'cluster','category', 'comb08', 15, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pivot_heatmap(vehicles, 2016, ['engine_size', 'fuel_efficiency'],'category', 'comb08', 15, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pivot_heatmap(vehicles, 2016, 'make','category', 'comb08', 10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Aggregations Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multi_line(df, x, y):\n",
    "    ax = df.groupby([x, y]).size().unstack(y).plot(figsize=(15,8), cmap=\"Set2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "multi_line(vehicles, 'year', 'category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bmw = vehicles[vehicles.make == 'BMW']\n",
    "multi_line(bmw, 'year', 'category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toyota = vehicles[vehicles.make == 'Toyota']\n",
    "multi_line(toyota, 'year', 'category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Field Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scatter_matrix(df, labels=None):\n",
    "    ax = sns.pairplot(df, hue=labels, diag_kind='kde', size=2)\n",
    "    plt.show()\n",
    "\n",
    "scatter_matrix(vehicles_numeric_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scatter_matrix(vehicles_numeric_norm, labels=\"cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vehicles_numeric_norm['Cluster'] = vehicles['cluster']\n",
    "sns.lmplot('displ', 'comb08', data=vehicles_numeric_norm, hue='Cluster', size=8, fit_reg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.lmplot('displ', 'fuelCost08', data=vehicles_numeric_norm, hue='Cluster', size=8, fit_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Entity Relationships (Graph Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entity = 'make'\n",
    "year = 2016\n",
    "\n",
    "vehicles_year = vehicles[vehicles.year==year]\n",
    "\n",
    "graph_year = pd.DataFrame(vehicles_year.groupby([entity,'cylinders','displ','trantype','drive',\n",
    "                                                 'comb08','VClass', 'cluster'], \n",
    "                                                as_index=False).size()).reset_index()\n",
    "\n",
    "graph_year = graph_year.rename(columns={0: 'count'})\n",
    "graph_year['edge'] = (graph_year['cylinders'].map(str)\n",
    "                      + graph_year['displ'].map(str)\n",
    "                      + graph_year['trantype']\n",
    "                      + graph_year['drive']\n",
    "                      + graph_year['comb08'].map(str)\n",
    "                      + graph_year['VClass']\n",
    "                      + graph_year['cluster']\n",
    "                     )\n",
    "\n",
    "graph_year = graph_year[[entity, 'edge', 'count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df_to_graph(df, entity, edge):\n",
    "    df2 = df.copy()\n",
    "    graph_df = pd.merge(df, df2, how='inner', on=edge)\n",
    "    graph_df = graph_df.groupby([entity + '_x', entity + '_y']).count().reset_index()\n",
    "    graph_df = graph_df[graph_df[entity + '_x'] != graph_df[entity + '_y']]\n",
    "    graph_df = graph_df[[entity + '_x', entity + '_y', edge]]\n",
    "    return graph_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vehicle_make_graph = df_to_graph(graph_year, entity, 'edge')\n",
    "vehicle_make_graph.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import graph_tool.all as gt\n",
    "import graph_tool as gt\n",
    "from graph_tool import *\n",
    "\n",
    "G = nx.from_pandas_dataframe(vehicle_make_graph, entity + '_x', entity + '_y', 'edge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Converting NetworkX to Graph-Tool](http://bbengfort.github.io/snippets/2016/06/23/graph-tool-from-networkx.html) by Benjamin Bengfort (converts NetworkX graphs to much prettier Graph-Tool graphs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prop_type(value, key=None):\n",
    "    \"\"\"\n",
    "    Performs typing and value conversion for the graph_tool PropertyMap class.\n",
    "    If a key is provided, it also ensures the key is in a format that can be\n",
    "    used with the PropertyMap. Returns a tuple, (type name, value, key)\n",
    "    \"\"\"\n",
    "    if isinstance(key, unicode):\n",
    "        # Encode the key as ASCII\n",
    "        key = key.encode('ascii', errors='replace')\n",
    "\n",
    "    # Deal with the value\n",
    "    if isinstance(value, bool):\n",
    "        tname = 'bool'\n",
    "\n",
    "    elif isinstance(value, int):\n",
    "        tname = 'float'\n",
    "        value = float(value)\n",
    "\n",
    "    elif isinstance(value, float):\n",
    "        tname = 'float'\n",
    "\n",
    "    elif isinstance(value, unicode):\n",
    "        tname = 'string'\n",
    "        value = value.encode('ascii', errors='replace')\n",
    "\n",
    "    elif isinstance(value, dict):\n",
    "        tname = 'object'\n",
    "\n",
    "    else:\n",
    "        tname = 'string'\n",
    "        value = str(value)\n",
    "\n",
    "    return tname, value, key\n",
    "\n",
    "\n",
    "def nx2gt(nxG):\n",
    "    \"\"\"\n",
    "    Converts a networkx graph to a graph-tool graph.\n",
    "    \"\"\"\n",
    "    # Phase 0: Create a directed or undirected graph-tool Graph\n",
    "    gtG = gt.Graph(directed=nxG.is_directed())\n",
    "\n",
    "    # Add the Graph properties as \"internal properties\"\n",
    "    for key, value in nxG.graph.items():\n",
    "        # Convert the value and key into a type for graph-tool\n",
    "        tname, value, key = get_prop_type(value, key)\n",
    "\n",
    "        prop = gtG.new_graph_property(tname) # Create the PropertyMap\n",
    "        gtG.graph_properties[key] = prop     # Set the PropertyMap\n",
    "        gtG.graph_properties[key] = value    # Set the actual value\n",
    "\n",
    "    # Phase 1: Add the vertex and edge property maps\n",
    "    # Go through all nodes and edges and add seen properties\n",
    "    # Add the node properties first\n",
    "    nprops = set() # cache keys to only add properties once\n",
    "    for node, data in nxG.nodes_iter(data=True):\n",
    "\n",
    "        # Go through all the properties if not seen and add them.\n",
    "        for key, val in data.items():\n",
    "            if key in nprops: continue # Skip properties already added\n",
    "\n",
    "            # Convert the value and key into a type for graph-tool\n",
    "            tname, _, key  = get_prop_type(val, key)\n",
    "\n",
    "            prop = gtG.new_vertex_property(tname) # Create the PropertyMap\n",
    "            gtG.vertex_properties[key] = prop     # Set the PropertyMap\n",
    "\n",
    "            # Add the key to the already seen properties\n",
    "            nprops.add(key)\n",
    "\n",
    "    # Also add the node id: in NetworkX a node can be any hashable type, but\n",
    "    # in graph-tool node are defined as indices. So we capture any strings\n",
    "    # in a special PropertyMap called 'id' -- modify as needed!\n",
    "    gtG.vertex_properties['id'] = gtG.new_vertex_property('string')\n",
    "\n",
    "    # Add the edge properties second\n",
    "    eprops = set() # cache keys to only add properties once\n",
    "    for src, dst, data in nxG.edges_iter(data=True):\n",
    "\n",
    "        # Go through all the edge properties if not seen and add them.\n",
    "        for key, val in data.items():\n",
    "            if key in eprops: continue # Skip properties already added\n",
    "\n",
    "            # Convert the value and key into a type for graph-tool\n",
    "            tname, _, key = get_prop_type(val, key)\n",
    "\n",
    "            prop = gtG.new_edge_property(tname) # Create the PropertyMap\n",
    "            gtG.edge_properties[key] = prop     # Set the PropertyMap\n",
    "\n",
    "            # Add the key to the already seen properties\n",
    "            eprops.add(key)\n",
    "\n",
    "    # Phase 2: Actually add all the nodes and vertices with their properties\n",
    "    # Add the nodes\n",
    "    vertices = {} # vertex mapping for tracking edges later\n",
    "    for node, data in nxG.nodes_iter(data=True):\n",
    "\n",
    "        # Create the vertex and annotate for our edges later\n",
    "        v = gtG.add_vertex()\n",
    "        vertices[node] = v\n",
    "\n",
    "        # Set the vertex properties, not forgetting the id property\n",
    "        data['id'] = str(node)\n",
    "        for key, value in data.items():\n",
    "            gtG.vp[key][v] = value # vp is short for vertex_properties\n",
    "\n",
    "    # Add the edges\n",
    "    for src, dst, data in nxG.edges_iter(data=True):\n",
    "\n",
    "        # Look up the vertex structs from our vertices mapping and add edge.\n",
    "        e = gtG.add_edge(vertices[src], vertices[dst])\n",
    "\n",
    "        # Add the edge properties\n",
    "        for key, value in data.items():\n",
    "            gtG.ep[key][e] = value # ep is short for edge_properties\n",
    "\n",
    "    # Done, finally!\n",
    "    return gtG\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Create the networkx graph\n",
    "    nxG = nx.Graph(name=\"Undirected Graph\")\n",
    "    nxG.add_node(\"v1\", name=\"alpha\", color=\"red\")\n",
    "    nxG.add_node(\"v2\", name=\"bravo\", color=\"blue\")\n",
    "    nxG.add_node(\"v3\", name=\"charlie\", color=\"blue\")\n",
    "    nxG.add_node(\"v4\", name=\"hub\", color=\"purple\")\n",
    "    nxG.add_node(\"v5\", name=\"delta\", color=\"red\")\n",
    "    nxG.add_node(\"v6\", name=\"echo\", color=\"red\")\n",
    "\n",
    "    nxG.add_edge(\"v1\", \"v2\", weight=0.5, label=\"follows\")\n",
    "    nxG.add_edge(\"v1\", \"v3\", weight=0.25, label=\"follows\")\n",
    "    nxG.add_edge(\"v2\", \"v4\", weight=0.05, label=\"follows\")\n",
    "    nxG.add_edge(\"v3\", \"v4\", weight=0.35, label=\"follows\")\n",
    "    nxG.add_edge(\"v5\", \"v4\", weight=0.65, label=\"follows\")\n",
    "    nxG.add_edge(\"v6\", \"v4\", weight=0.53, label=\"follows\")\n",
    "    nxG.add_edge(\"v5\", \"v6\", weight=0.21, label=\"follows\")\n",
    "\n",
    "    for item in nxG.edges_iter(data=True):\n",
    "        print(item)\n",
    "\n",
    "    # Convert to graph-tool graph\n",
    "    gtG = nx2gt(nxG)\n",
    "    gtG.list_properties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_graph(graph, width, length):\n",
    "    g = nx2gt(graph)\n",
    "    vlabel = g.vp['id']\n",
    "    gt.graph_draw(g, output_size=(width,length), vertex_text=vlabel, vertex_font_weight=0.2, \n",
    "               vertex_size=5, vertex_fill_color='cyan')\n",
    "\n",
    "plot_graph(G, 1200, 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ego = nx.ego_graph(G, 'Nissan', 1)\n",
    "plot_graph(ego, 500, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import community\n",
    "\n",
    "def detect_communities(graph):\n",
    "    partition = community.best_partition(graph)\n",
    "    nx.set_node_attributes(graph, 'partition', partition)\n",
    "    return graph, partition\n",
    "\n",
    "make_communities = pd.DataFrame(detect_communities(G)[1].items(), \n",
    "                                columns=['make', 'community']).sort_values('community', ascending=True)\n",
    "\n",
    "make_communities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from copy import copy\n",
    "\n",
    "##########################################################################\n",
    "## Color Palettes\n",
    "##########################################################################\n",
    "\n",
    "FLATUI = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\n",
    "PAIRED = [\n",
    "    \"#a6cee3\", \"#1f78b4\", \"#b2df8a\", \"#33a02c\", \"#fb9a99\", \"#e31a1c\",\n",
    "    \"#fdbf6f\", \"#ff7f00\", \"#cab2d6\", \"#6a3d9a\", \"#ffff99\", \"#b15928\",\n",
    "]\n",
    "SET1   = [\n",
    "    \"#e41a1c\", \"#377eb8\", \"#4daf4a\",\n",
    "    \"#984ea3\", \"#ff7f00\", \"#ffff33\",\n",
    "    \"#a65628\", \"#f781bf\", \"#999999\"\n",
    "]\n",
    "\n",
    "PALETTES = {\n",
    "    'flatui': FLATUI,\n",
    "    'paired': PAIRED,\n",
    "    'set1': SET1,\n",
    "}\n",
    "\n",
    "##########################################################################\n",
    "## Color Utilities\n",
    "##########################################################################\n",
    "\n",
    "class ColorMap(object):\n",
    "    \"\"\"\n",
    "    A helper for mapping categorical values to colors on demand.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, colors='flatui', shuffle=False):\n",
    "        \"\"\"\n",
    "        Specify either a list of colors or one of the color names. If shuffle\n",
    "        is True then the colors will be shuffled randomly.\n",
    "        \"\"\"\n",
    "        self.mapping = {}\n",
    "        self.colors = colors\n",
    "\n",
    "        if shuffle:\n",
    "            random.shuffle(self._colors)\n",
    "\n",
    "    @property\n",
    "    def colors(self):\n",
    "        return self._colors\n",
    "\n",
    "    @colors.setter\n",
    "    def colors(self, value):\n",
    "        \"\"\"\n",
    "        Converts color strings into a color listing.\n",
    "        \"\"\"\n",
    "        if isinstance(value, basestring):\n",
    "            if value not in PALETTES:\n",
    "                raise ValueError(\"'{}' is not a registered color palette\")\n",
    "            self._colors = copy(PALETTES[value])\n",
    "        elif isinstance(value, list):\n",
    "            self._colors = value\n",
    "        else:\n",
    "            self._colors = list(value)\n",
    "\n",
    "    def __call__(self, category):\n",
    "        if category not in self.mapping:\n",
    "            if self.colors:\n",
    "                self.mapping[category] = self.colors.pop()\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Not enough colors for this many categories!\"\n",
    "                )\n",
    "\n",
    "        return self.mapping[category]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_community_graph(graph, community_df, width, length):\n",
    "    g = nx2gt(G)\n",
    "    vlabel = g.vp['id']\n",
    "    vcolor = g.new_vertex_property('string') \n",
    "    vcmap = ColorMap('flatui', shuffle=False)\n",
    "    for vertex in g.vertices():\n",
    "        vcolor[vertex] = vcmap(community_df.community[vertex])\n",
    "    gt.graph_draw(g, output_size=(width,length), vertex_text=vlabel, vertex_font_weight=0.2, \n",
    "               vertex_size=5, vertex_fill_color=vcolor)\n",
    "\n",
    "plot_community_graph(G, make_communities, 1200, 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Connections Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = ['make_x','make_y', 'edge','year']\n",
    "graph_all_years = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in vehicles['year'].unique():\n",
    "    vehicles_year = vehicles[vehicles.year==i]\n",
    "\n",
    "    graph_year = pd.DataFrame(vehicles_year.groupby([entity,'cylinders','displ','trantype','drive',\n",
    "                                                     'comb08','VClass', 'cluster'], \n",
    "                                                    as_index=False).size()).reset_index()\n",
    "\n",
    "    graph_year = graph_year.rename(columns={0: 'count'})\n",
    "    graph_year['edge'] = (graph_year['cylinders'].map(str)\n",
    "                          + graph_year['displ'].map(str)\n",
    "                          + graph_year['trantype']\n",
    "                          + graph_year['drive']\n",
    "                          + graph_year['comb08'].map(str)\n",
    "                          + graph_year['VClass']\n",
    "                          + graph_year['cluster']\n",
    "                         )\n",
    "\n",
    "    graph_year = graph_year[[entity, 'edge', 'count']]\n",
    "    vehicle_make_graph = df_to_graph(graph_year, entity, 'edge')\n",
    "    vehicle_make_graph['year'] = i\n",
    "    graph_all_years = graph_all_years.append(vehicle_make_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_summary = graph_all_years.groupby(['make_x', 'year'], \n",
    "                                        as_index=False).sum()\n",
    "\n",
    "graph_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def graph_multi_line(df, x, y):\n",
    "    ax = df.groupby([x, y]).sum().unstack(y).plot(figsize=(15,8), cmap=\"jet\")\n",
    "    ax.legend(loc='center', bbox_to_anchor=(0.5, -0.35),\n",
    "          ncol=5, fancybox=True, shadow=True, labels=df[y].unique())\n",
    "\n",
    "graph_multi_line(graph_summary, 'year', 'make_x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "makes = ['Chevrolet', 'Ford', 'Toyota', 'Honda', 'Nissan']\n",
    "\n",
    "def graph_multi_line_makes(df, x, y):\n",
    "    ax = df.groupby([x, y]).sum().unstack(y).plot(figsize=(15,8), cmap=\"jet\")\n",
    "    ax.legend(loc='center', bbox_to_anchor=(0.5, -0.15),\n",
    "          ncol=5, fancybox=True, shadow=True, labels=df[y].unique())\n",
    "\n",
    "graph_summary_makes = graph_summary[graph_summary.make_x.isin(makes)]\n",
    "graph_multi_line_makes(graph_summary_makes, 'year', 'make_x')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
